---
title: 斯坦福大学—机器学习笔记（二）
date: 2017-09-24 22:18:55
tags: [线性回归, 梯度下降法, 代价函数, 正规方程]
categories: 机器学习
---
该部分笔记主要记录《机器学习》视频中的第一个算法：**线性回归算法**

包含视频内容范围为第二和第四周视频，主要内容分为以下2部分：

1. 单变量线性回归
2. 多变量线性回归

# 单变量线性回归

某个目标量可能由一个或多个变量决定，单变量线性回归就是我们仅考虑一个变量与目标量的关系。例如，我们可以仅考虑房子的面积X与房价y的关系

## 模型表示
预测房价是一个关于监督学习的回归问题，该问题的训练集表示如下：
![](http://i.imgur.com/79Z8Fk4.jpg)

其中：

- x表示输入变量/特征
- y表示输出变量/目标变量
- m表示训练集中的实例的数量

对于预测房价问题，学习过程如下：
![](http://i.imgur.com/5wqtIQj.jpg)

对于单变量线性回归模型，操作如下：

1. 定义假设函数
2. 利用大量的训练集（已标定结果的数据集）训练得出参数
3. 预测给定输入样本的输出值

在上述操作中，我们需要知道如何评价我们的假设函数的可靠性呢？

于是定义出Cost Function J，为了使得假设函数接近或等于实际值，目标是使得函数J取最小值。在线性回归模型中，Cost Function表示为：\\( J(\theta\_{0}, \theta\_{1}) = \frac{1}{2m}\sum\_{i=1}^m{(h\_{\theta}(x^{(i)}) - y^{(i)})^2} \\)，但这并不代表所有的模型都采用该表示法。

注：Cost Function式中的1/2m只是为了简化之后的求导计算

## 代价函数（Cost Function）

如何确定函数h(x)表达式中的参数\\( \theta\_{i} \\)呢？
在房价问题这个例子中,我们选择的参数决定了我们得到的直线相对于我们的训练集的准确程度，模型所预测的值与训练集中实际值之间的差值表示建模误差。
![](http://i.imgur.com/uFcna8T.jpg)

注：上图，蓝线表示建模误差。

建模误差的平方和最小值（即代价函数）所对应的模型参数就是我们所要确定的参数。

### 代价函数解释一

关于预测房价的线性回归主要包含：

- Hypothesis： \\( h\_{\theta}(x)=\theta\_{0}+\theta\_{1}x \\)
- Parameters： \\( \theta\_{0}，\theta\_{1} \\)
- Cost Function： \\( J(\theta\_{0}, \theta\_{1}) = \frac{1}{2m}\sum\_{i=1}^m{(h\_{\theta}(x^{(i)}) - y^{(i)})^2} \\)
- Goal： \\( minimize J(\theta\_{0}, \theta\_{1}) \\)

简化模型：令\\( \theta\_{0} \\)=0，然后令\\( \theta\_{1} \\)分别取1、0.5、-0.5等值，做出下图：
![](http://i.imgur.com/QGuxaaC.jpg)

当\\( \theta\_{0} \\)=0且\\( \theta\_{1} \\)=1时，代价函数最小。

### 代价函数解释二

当我们不再对代价函数做简化时，\\( J(\theta\_{0}, \theta\_{1}) \\)的图形如下所示：
![](http://i.imgur.com/c8oSGbF.jpg)

我们需要在三维空间中寻找一个使得\\( J(\theta\_{0}, \theta\_{1}) \\)最小的点。该点的坐标就是我们需要确定的模型参数。我们需要一种有效的算法能够自动找出这些使代价函数J取最小值时的参数。该算法就是接下来要介绍的梯度下降法

## 梯度下降法

梯度下降算法用来求解函数最小值。它的基本思想是：开始时我们随机选择一个参数的组合（θ0,θ1,...,θn），计算代价函数，然后我们寻找下一个能让代价函数值下降最多的参数组合。我们持续这么做直到一个局部最小值。对于初始值的位置不同，寻找的最小值也不一样。
![](http://i.imgur.com/DY8Qvl0.jpg)

梯度下降算法表达式如下所示：
![](http://i.imgur.com/GaWB16k.jpg)

其中：

- \\( \alpha \\)表示学习率，它决定了代价函数下降程度最大方向下的速率

参数更新过程需要同步，即
![](http://i.imgur.com/FDWI9Pg.jpg)

所以，本课程所用的梯度下降算法又称批量梯度下降法。

### 梯度下降解释

对于一个简化的\\( J(\theta\_{1}) \\)最来说，无论抛物线的左边还是右边，在梯度下降算法下，\\( \theta\_{1} \\)都是保持正确的方向（递增或递减）

如果\\( \alpha \\)过小，梯度下降可能很慢；如果过大，梯度下降有可能错过最小点，并且有可能收敛失败。

## 梯度下降的线性回归

线性回归模型：
![](http://i.imgur.com/MQsIA7Q.jpg)

梯度下降算法：
![](http://i.imgur.com/LabceET.jpg)

对于线性回归模型采用梯度下降算法关键在于求解代价函数的导数，即：
![](http://i.imgur.com/HzOp1lb.jpg)

在梯度下降算法中对代价函数的偏导进行替换，就得到单变量线性回归梯度下降算法：
![](http://i.imgur.com/FOiBxHg.jpg)

## 关于梯度下降法的一些注意点

1. 迭代次数：
 - 过少可能使得算法还没有收敛就停止，
 - 过多导致资源（时间等）的浪费；
2. learning rate:
 - 过小，使得每次迭代时theta的变化量过小，从而算法收敛过慢，换言之需要增加迭代次数使得算法收敛；
 - 过大，使得每次迭代时theta的变化量过大，可能在变化（迭代）过程中越过最优（收敛）点。

所以，一般在调整参数时，最好画出变化趋势图，对此进行分析
![](https://i.imgur.com/sUBWw4H.png)

# 多变量线性回归

实际生活中，预测房价模型不会只考虑一个变量的影响，需要考虑多个变量的影响，才能更好的帮助我们做出更准确的预测。
![](https://i.imgur.com/016cEQ8.png)

在单变量线性回归模型中，n=1，而在多变量线性回归模型中，n从1增加到n。对应的\\( \theta \\)也从2维变成n+1维。于是，预测函数也发生了变化
![](https://i.imgur.com/NBmOXje.png)

同样地，梯度下降算法仍适用于该模型，单变量线性回归可以看作是多变量线性回归n=1的特殊情况：
![](https://i.imgur.com/N4JVpHS.png)

## 数据规范化

在单变量线性回归中，变量只有一个，不存在数据是否规范化问题，但是，在多变量线性回归中，不同的变量量由于单位不同，导致可能在数值上相差较大，对于梯度下降算法求解过程带来一定的影响，比如：影响梯度下降法的迭代次数。所以，我们需要数据规范(Feature Scaling)。

![](https://i.imgur.com/gmhYXFs.png)

一种常见的规范化策略:

1. 求每个特征量X的平均值mean
2. 求每个特征量X的标准差segma   
3. 规范化：X = (X-mean) / sigma

## 正规方程
该部分暂时不做解释，等有时间写出推导过程，目前只给出结果
\\( \theta =\left( X^{T}X\right) ^{-1}X^{T}y \\)
## 梯度下降算法VS正规方程
![](https://i.imgur.com/h9YTVeF.png)



