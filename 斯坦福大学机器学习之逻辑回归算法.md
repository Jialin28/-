# 简述
该部分笔记主要记录《机器学习》视频中的第二个算法：**逻辑回归算法**
包含视频内容范围为第六周视频，主要内容分为以下部分：
1. 二分类问题
2. 逻辑回归模型
3. 决策边界
4. 代价函数
5. 多分类问题

# Classification(分类)
以下引自李航博士《统计学习方法》1.8节关于分类问题的一点描述：
> 分类是监督学习的一个核心问题，在监督学习中，当输出变量Y取有限个离散值时，预测问题便成为分类问题。这时，输入变量X可以是离散的，也可以是连续的。监督学习从数据中学习一个分类模型或分类决策函数，称为分类器(classifier)。分类器对新的输入进行输出的预测(prediction)，称为分类(classification)。

常见的二分类问题：
1. 邮件：垃圾邮件/非垃圾邮件？
2. 在线交易：是否欺诈（是/否）？
3. 肿瘤：恶性/良性？

采用数学模型表示为：
二分类：\\( y \in \{0, 1, 2\} \\)
多分类：\\( y \in \{0, 1, 2, 3, ..., n\} \\)
对于肿瘤这个二分类问题，我们的训练集表示如下：
![](https://i.imgur.com/pSepvvZ.png)

我们可以使用之前的线性回归模型对其进行预处理，然后对其输出设置一个阈值：
1. 如果\\( h_\theta(x) \geq 0.5 \\)，则预测y=1,即y属于正例；
2. 如果\\( h_\theta(x) < 0.5 \\)，则预测y=0,即y属于负例；
![](https://i.imgur.com/1OYuVQZ.png)

但是线性回归模型的Hypothesis输出值\\( h\_\theta(x) \\)可以大于1也可以小于0。对之后的处理会造成一定的影响，为了解决这个问题，我们引入**逻辑回归模型**，该模型将利用一个函数使Hypothesis输出介于0与1之间。

# Logistic Regression(逻辑回归模型)
在逻辑回归模型中，我们需要将Hypothesis的输出界定在0和1之间，即：
$$ 0 \leq h\_\theta(x) \leq 1 $$
我们引入函数g可以实现该输出。函数g被称为Sigmoid function或者Logistic function，表达式如下：
$$ g(z) = \frac{1}{1+e^{-z}} $$
其图形表示为：
![](https://i.imgur.com/EC4tZsi.png)

令逻辑回归的Hypothesis表示为：
$$ h\_\theta(x) = g(\theta^T x) = \frac{1}{1+e^{-\theta^Tx}} $$
Hypothesis输出函数解释：\\( h\_\theta(x) \\) = 对于给定的输入x，y=1时估计的概率
例如，对于肿瘤（恶性/良性），如果输入变量（特征）是肿瘤的大小：
![](https://i.imgur.com/gANrPBa.png)

这里Hypothesis表示的是”病人的肿瘤有70%的可能是恶性的“。较正式的说法可以如下表示：给定输入x，参数化的θ（参数空间）， y=1时的概率。
数学上可以如下表示：
$$ h\_\theta(x) = P(y=1| x;\theta) $$

# Decision boundary(决策边界)
在逻辑回归模型中，假设给定的阈值是0.5
- 当\\( h_\theta(x) \geq 0.5 \\)时， y = 1;
- 当\\( h_\theta(x) < 0.5 \\)时，y = 0;

结合sigmoid function的图形，我们发现：
当\\( g(z) \geq 0.5 \\)时，\\( z \geq 0 \\)，即对于\\( h\_\theta(x) = g(\theta^Tx) \geq 0.5 \\)，则\\( \theta^Tx \geq 0 \\)，此时意味着y=1；反之，对于\\( h\_\theta(x) = g(\theta^Tx) < 0.5 \\)，则\\( \theta^Tx < 0 \\)，此时意味着y=1。
我们可以认为\\( \theta^Tx \\)是一个决策边界，当它大于0或小于0时，逻辑回归模型分别预测不同的分类结果。
例如，我们现在有一个二分类问题：其Hypothesis函数表示为：\\( h\_\theta(x) = g(\theta\_0 + \theta\_1 x\_1 + \theta\_2 x_2) \\)。我们通过训练得到其参数\\( \theta\_0, \theta\_1, \theta\_2 \\)分别等于-3，1，1。
当\\( -3 + x\_1 + x\_2 \geq 0 \\)时，\\( y = 1 \\)；当\\( -3 + x\_1 + x\_2 < 0 \\)时，\\( y = 0 \\)，则\\( x\_1 + x\_2 = 3 \\)表示一个决策边界。
决策边界图形表示如下：
![](https://i.imgur.com/LGvrvfg.png)

# Cost function(代价函数)
线性回归的Cost  Function定义为:
$$ J(\theta) = \frac{1}{m}\sum\_{i=1}^m{\frac{1}{2}(h\_\theta(x^{(i)}) - y^{(i)})^2} $$
为了简单起见，将\\( \frac{1}{2}(h\_\theta(x^{(i)}) - y^{(i)})^2 \\)表示为\\( Cost(h\_\theta(x^{(i)}, y) \\)
如果和线性回归相似，这里取\\( h\_\theta(x) = \frac{1}{1+e^{-\theta^Tx}} \\)会存在一个问题，也就是逻辑回归的Cost Function是“非凸”的，如下图所示：
![](https://i.imgur.com/y1f1h2n.png)

非凸函数存在多个局部值，会影响全局最小值点的判断，从而影响模型中参数的预测，而凸函数具有良好的性质：对于凸函数来说局部最小值点即为全局最小值点，因此只要能求得这类函数的一个最小值点，该点一定为全局最小值点。
![](https://i.imgur.com/n1mkK5y.png)

因此，我们需要其他形式的代价函数来进行优化。
![](https://i.imgur.com/5ZeVfml.png)

## 代价函数解释
y = 1的情况：
![](https://i.imgur.com/9Prs758.png)

如果\\( y = 1 \\)，\\( h\_\theta(x) = 1 \\)，则\\( Cost = 1 \\)，也就是预测的值和真实的值完全相等
但是，当\\( h\_\theta(x) \to 0 \\)时，\\( Cost \to \infty \\)，预测结果出现错误，即\\( h\_\theta(x) = 0 \\)，也就是预测\\( P(y = 1|x; \theta) = 0 \\)，也就是y = 1的概率是0，但是实际上y = 1
y = 0的情况类似，不做叙述。

在数学中，一个分段函数在能够写成一个统一的表达式时，我们会尽量不写分段表示法，所以我们的损失函数可以写成以下形式：
![](https://i.imgur.com/0ce1Ild.png)

在逻辑回归模型中，我们求解参数的方法和线性回归模型中使用到的方法一样，也是利用梯度下降算法
![](https://i.imgur.com/JSyl0EA.png)

除了使用梯度算法之外，一般还使用如下算法进行处理，如：
1. Conjugate gradient(共轭梯度法)
2. BFGS
3. L-BFGS

上述三种算法和梯度算法相比：
- 优势:
  - 无需选择Learning rate
  - 计算速度快
- 劣势:
  - 相对复杂

# 多分类问题
对于多分类问题来说，我们一般是将其拆分为二分类问题进行处理的
![](https://i.imgur.com/3Sfwwd8.png)

参考链接：
- [Coursera公开课笔记: 斯坦福大学机器学习第六课“逻辑回归(Logistic Regression)”](http://www.52nlp.cn/coursera%E5%85%AC%E5%BC%80%E8%AF%BE%E7%AC%94%E8%AE%B0-%E6%96%AF%E5%9D%A6%E7%A6%8F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%AC%E5%85%AD%E8%AF%BE%E9%80%BB%E8%BE%91%E5%9B%9E) 
- [逻辑回归-课件](https://d19vezwu8eufl6.cloudfront.net/ml/docs%2Fslides%2FLecture6.pdf)
- [BFGS算法](http://blog.csdn.net/acdreamers/article/details/44664941)
- [L-BFGS 算法](http://blog.csdn.net/itplus/article/details/21897715)
- [机器学习常见的最优化算法](http://www.cnblogs.com/hlongch/p/5734105.html)


