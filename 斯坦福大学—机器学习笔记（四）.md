---
title: 斯坦福大学—机器学习笔记（四）
date: 2017-10-07 18:43:40
tags: 正则化
categories: 机器学习
---
该部分笔记主要记录《机器学习》第七周视频中的：**正则化**

# Overfitting(过拟合)
1) 线性回归
![](https://i.imgur.com/OVSUFva.png)
2） 逻辑回归
![](https://i.imgur.com/yoEgk5f.png)
这两幅图可以比较容易地看出：
1. 左图中的函数拟合的有点欠缺 ——> **欠拟合**
2. 中图中的函数拟合较左图有了明显改善 ——> 正常
3. 右图中的函数拟合的接近完美 ——> **过拟合**

右图中这种情况难道不是我们最希望出现的状况吗？为什么还被成为过拟合呢？很简单，这种情况下训练出的模型只是适合于自己这个测试用例，对于其它的测试集表现不尽人意。也就是说，过拟合是指预测函数在训练集上表现非常好，但在测试集上表现差的情况。

**如何解决过拟合问题呢？**
过拟合出现的真正原因在于预测函数过分地注意过多的特征，想要满足所有的特征表示。所以针对过拟合问题，通常会考虑两种途径来解决：
1. 减少特征的数量
2. 正则化

对于减少特征的数量这种方法来说，需要人工的去选择哪些特征或者一些选择算法来完成，但是这些特征最后都会对你的预测或多或少的产生影响，直接减少特征数量有些不妥；
对于正则化这种方法来说，将会保留所有的特征，但是降低参数\\( \theta\_j \\)的量/值,正则化的好处是当特征很多时，每一个特征都会对预测贡献一份合适的力量。

进行了正则化的Cost Function如下所示：
![](https://i.imgur.com/JufANxD.png)

简单来说，正则化就是对Cost Function中的某些特征进行“惩罚”。

参考链接：
[机器学习中常常提到的正则化到底是什么意思？](https://www.zhihu.com/question/20924039)





